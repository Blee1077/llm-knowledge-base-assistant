{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pickle\n",
    "import json\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import PreProcessor, EmbeddingRetriever\n",
    "from haystack.utils import convert_files_to_docs\n",
    "\n",
    "\n",
    "def load_json_file(file_path) -> dict:\n",
    "    \"\"\"Load a local JSON file into a dictionary in Python.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file to load.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the contents of the JSON file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the file contents cannot be parsed as valid JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the JSON file and load its contents into a Python dictionary\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid JSON in file: {file_path}\")\n",
    "\n",
    "\n",
    "def load_json_from_s3(bucket: str, key: str) -> dict:\n",
    "    \"\"\"Loads a JSON file from S3 bucket.\n",
    "    \n",
    "    Args:\n",
    "        bucket (str): S3 bucket containing JSON file\n",
    "        key (str): Path within bucket of JSON file\n",
    "        \n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    content_object = S3_RESOURCE.Object(bucket, key)\n",
    "    file_content = content_object.get()[\"Body\"].read().decode(\"utf-8\")\n",
    "    return json.loads(file_content)\n",
    "\n",
    "\n",
    "S3_RESOURCE = boto3.resource('s3')\n",
    "S3_CONFIG = load_json_file('../s3_config.json')\n",
    "OPENAI_API_KEY = load_json_from_s3(bucket=S3_CONFIG['S3_BUCKET'], key=S3_CONFIG[\"OPENAI_API_S3_KEY\"])['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3bc55996a84febac0525c2e6883ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/11 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_dir = \"data\"\n",
    "files_to_index = [doc_dir + \"/\" + f for f in os.listdir(doc_dir)]\n",
    "\n",
    "# Set up document store that splits the documents into segments\n",
    "split_doc_store = InMemoryDocumentStore(embedding_dim=1536)\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_length=200,\n",
    "    split_overlap=10,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "raw_docs = convert_files_to_docs(dir_path=doc_dir)\n",
    "proc_docs = preprocessor.process(raw_docs)\n",
    "split_doc_store.write_documents(proc_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_doc_store = InMemoryDocumentStore(embedding_dim=1536)\n",
    "whole_docs = convert_files_to_docs(dir_path=doc_dir)\n",
    "whole_doc_store.write_documents(whole_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55834d56903a4aca8ab8ee1e9dcc0319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/77 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1954d72cd7ec4984bbc6b7fa091b6ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating embeddings:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OpenAI EmbeddingRetriever\n",
    "split_doc_retriever = EmbeddingRetriever(\n",
    "    document_store=split_doc_store,\n",
    "    batch_size=8,\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    max_seq_len=8192,\n",
    "    top_k=4\n",
    ")\n",
    "\n",
    "split_doc_store.update_embeddings(split_doc_retriever)\n",
    "\n",
    "with open('split_doc_store.pkl', 'wb') as f:\n",
    "    pickle.dump(split_doc_store, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a16344bcfc45d1ac3ea2cb5ecd6f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/11 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93747175d654b139fd7d9f9d6dadf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "whole_doc_retriever = EmbeddingRetriever(\n",
    "    document_store=whole_doc_store,\n",
    "    batch_size=8,\n",
    "    embedding_model=\"text-embedding-ada-002\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    max_seq_len=8192,\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "whole_doc_store.update_embeddings(whole_doc_retriever)\n",
    "\n",
    "with open('whole_doc_store.pkl', 'wb') as f:\n",
    "    pickle.dump(whole_doc_store, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_assist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
